# Introduction to Algorithmic Efficiency (Complexity) ğŸ“š 

Now that we have developed an understanding of how some search/sort algorithms work, the next logical step is to examine just how well they work. In this section we will analyse the performance of algorithms. In computer science this is often referred to as algorithmic efficiency or complexity.


Two common measures of algorithmic efficiency are space and time.

**1. Space:** Provides an indication of the demands an algorithm places on memory in terms of space requirements.

**2. Time:**  Provides an indication of the time requirements of an algorithm.

For the most part, we will be confining the remainder of our discussion to time complexity.

## How to Compare Algorithms ğŸ‘¨ğŸ½â€ğŸ”¬
Before we can begin to compare algorithms in terms of their performance, we must first devise _(or at least agree upon)_ some system that is both impartial and reliable.

On the surface it might seem fair and make sense to simply time how long it takes an algorithm to run in minutes and seconds _(or milliseconds)_ and use this as a measure of performance. As it turns out however this would be neither fair nor reliable. 

This is because a computerâ€™s performance can depend on a variety of different factors _(e.g. processor clock speed, word size, bus width and amount of available memory)_, and so, an algorithm that takes **1000 milliseconds** to run on one computer might run in just **10 milliseconds** on another _(one hundred times faster!)_. 

In fact, depending on the processor load, the time taken to run an algorithm could potentially vary significantly from run to run on the same processor.

Also, the running time of an algorithm is likely to vary in accordance with the size of its input. Intuitively it is easy to understand that a particular sorting algorithm will sort **1,000** integers must faster than it will sort **1,000,000** integers. 

However, as we will soon learn to appreciate _(hopefully!)_, it is the specific techniques and _**nuances**_ employed by algorithms that have a much greater bearing on performance than the size of the input.

Finally, there are questions such as:
  - What is the fastest time an algorithm can run in _(i.e. what is the best case performance)_?
  - Or is there an average performance time for a particular algorithm?
  - What about a worst case?

As it turns out it is this final question _(regarding worst case)_ that computer scientists are most interested in. 

The reason is that a _**worst case running time**_ gives users a _**bottom line guarantee**_ that an algorithm will finish at worst within a particular timeframe, and for this reason worst case scenario is used as a metric for comparing algorithms.

## Big O ğŸ“šğŸ‘¨ğŸ½â€ğŸ’»
Big O is a notation used in Computer Science to describe the worst case running time _(or space requirements)_ of an algorithm in terms of the size of its input usually denoted by ``ğ‘›``.

![image](https://github.com/ross-bish/Algorithms/assets/83789503/a7b51721-258c-4318-b8f9-84d1a1198941)

Big O notation is important for several reasons:

- Big O Notation is important because it helps analyze the efficiency of algorithms.
- It provides a way to describe how the runtime or space requirements of an algorithm grow as the input size increases.
- Allows programmers to compare different algorithms and choose the most efficient one for a specific problem.
- Helps in understanding the scalability of algorithms and predicting how they will perform as the input size grows.
- Enables developers to optimize code and improve overall performance.

Lets take a look at some common examples of **Big O** values:

### _O( 1 )_
An algorithm described in this manner will always run within some constant time _(sometimes called bounded time)_ regardless of the size of the input.
Such algorithms are said to take _â€˜order of 1â€™_, or ``O(1)`` time to complete.

To take an analogy, letâ€™s say itâ€™s the weekend and you were preparing to do some serious study but before you get started you first need to clear your room/desk. The time required to do this work doesnâ€™t depend in any way on the number of subjects you intend to study. It will be completed within some **constant amount of time** regardless of whether you will study two or ten subjects.

### _O(n)_
If the length of time it takes to run an algorithm _increases _in proportion to the size of the input the algorithm is said to run in _linear time._ Such algorithms have an ``ğ‘‚(ğ‘›)`` complexity.

The _**linear (sequential) search**_ algorithm used to find some target value (argument) in a list that contains ``ğ‘›`` values is a classic example of an ``ğ‘‚(ğ‘›)`` algorithm. This is because in the worst case scenario every element in the list will have to be examined in order to find the target value.

These algorithms are characterised by the following loop structure.
````python
for i in range(n):
    print(i) # this line will be executed n times
````

Lets keep going with our earlier analogy â€“ let ``n`` be the number of subjects you are going to study and let us say that you had decided to allocate a fixed amount of time to each subject. It makes sense therefore that the more subjects you study the longer it will take to finish your study. Twice as many subjects will require twice the amount of time!

### _O(nÂ²)_
Now letâ€™s say that you decided to use a slightly different approach to your study. Instead of allocating the same fixed amount of time to each subject you decide to allocate fixed units of time to reading individual pages of notes. You start by reading one page for the first subject, two for the second, three for the third and so on. By the time you have reached your ``nth`` subject you will need to read ``ğ‘›`` pages of notes. The amount of time it takes to complete your study in this case is known as quadratic time and is written as ``O(ğ‘›Â²)``.

Algorithms of this type are characterised by _loops nested to one level._ 

  - For each of the ``ğ‘›`` iterations carried out by the outer loop, the inner loop will perform ``ğ‘›`` iterations of its own. 
  - This is illustrated in the snippet of Python code below in which the print statement appears within a nested for loop and will be executed ``ğ‘›Â²`` times.

````python
for i in range(n):
    for j in range(n):
      print(i, j) # this line will be executed n squared times
````
